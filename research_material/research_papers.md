# Soft Prompt Papers

- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691): This paper is the one that introduces prompt tuning. Leveraging continous soft prompts via backpropagation with a frozen pre-trained language model. Soft prompts can outperform GPT3's few shot performances.
- [Learning How to Ask: Querying LMs with Mixtures of Soft Prompts](https://arxiv.org/abs/2104.06599): this paper uses a mixture of soft prompts combined instead of using one fixed soft prompt. This is important because by having multiple soft prompts and learning a weighting function, the LLM can flexibly adapt to different tasks. For a given task, the model learns how to blend these prompts together. This is more powerful than relying on a single prompt embedding. The mixture of prompts outperform both hand-designed discrete prompts and the single learned soft prompts.
- [MetaPrompting: Learning to Learn Better Prompts](https://arxiv.org/abs/2209.11486): Solving the challenge of soft prompt initialization. This paper introduces a novel prompting method that uses meta-learning to create more effective soft prompts for LLMs in few shot learning scenarios. Learning a shared, general meta-knowledge initialization for prompts, MetaPrompting improves task adaptation. Achieves better adaptation, robustness, and performance compared to fine tuned soft prompt methods.
- [InfoPrompt: Information-Theoretic Soft Prompt Tuning](https://arxiv.org/abs/2306.04933): Addressing initialization sensitivity, traditional soft prompts tuning struggle because model performance is heavily depends on how the prompts are initialized and bad initialization can hamper results. InfoPrompts presents a information theoretic framework that accelerates convergence of training and leads to more robust prompt tuning. 
- [Soft Language Prompts for Language Transfer](https://arxiv.org/abs/2407.02317): Tackling the challenge of moving knowledge from high-resource language like English to low-resource languages. Existing approaches train language specific and task specific adapters, but often still fail. The paper introduces soft language prompts. Found that soft language prompts + task adapter wins more often. However, it is not always best for every task/language, some non-latin language like arabic and chinese shows negative transfer.

- [Towards Interpretable Soft Prompts](https://arxiv.org/abs/2504.02144): Proposes to make soft prompts more interpretable. Introduces a novel theoretical framework that defines interpretability through two key attributes: Faithfulness (interpretations should accurately reflect what the prompt is doing), scrutability (interpretations should be inspectable and meaningful to humans). Two advanced prompt tuning methods - PEZ(hard prompts made easy) and RLPrompt. Found that making soft prompts more interpretable often reduces task performance. This highlights that a fundamental challenge - interpretability and model effectiveness don't necessarily align.