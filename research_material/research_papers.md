# Soft Prompt Papers

- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691): This paper is the one that introduces prompt tuning. Leveraging continous soft prompts via backpropagation with a frozen pre-trained language model. Soft prompts can outperform GPT3's few shot performances.
- [Learning How to Ask: Querying LMs with Mixtures of Soft Prompts](https://arxiv.org/abs/2104.06599): this paper uses a mixture of soft prompts combined instead of using one fixed soft prompt. This is important because by having multiple soft prompts and learning a weighting function, the LLM can flexibly adapt to different tasks. For a given task, the model learns how to blend these prompts together. This is more powerful than relying on a single prompt embedding. The mixture of prompts outperform both hand-designed discrete prompts and the single learned soft prompts.
- [MetaPrompting: Learning to Learn Better Prompts](https://arxiv.org/abs/2209.11486): Solving the challenge of soft prompt initialization. This paper introduces a novel prompting method that uses meta-learning to create more effective soft prompts for LLMs in few shot learning scenarios. Learning a shared, general meta-knowledge initialization for prompts, MetaPrompting improves task adaptation. Achieves better adaptation, robustness, and performance compared to fine tuned soft prompt methods.
- [InfoPrompt: Information-Theoretic Soft Prompt Tuning](https://arxiv.org/abs/2306.04933): Addressing initialization sensitivity, traditional soft prompts tuning struggle because model performance is heavily depends on how the prompts are initialized and bad initialization can hamper results. InfoPrompts presents a information theoretic framework that accelerates convergence of training and leads to more robust prompt tuning. 
- [Soft Language Prompts for Language Transfer]
- [Towards Interpretable Soft Prompts]